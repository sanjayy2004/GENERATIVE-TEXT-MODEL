# GENERATIVE-TEXT-MODEL
COMPANY: CODETECH IT SOLUTIONS

NAME : SANJAY KURRA

INTERN ID: CTO4DN1694

DOMAIN : ARTIFICIAL INTELLIGENCE

DURATION :4 WEEKS

MMENTOR : NEELA SANTOSH

Project Description
This project showcases a text generation system using the pretrained GPT-2 model from Hugging Faceâ€™s transformers library. It generates coherent, paragraph-length text based on user-provided prompts.

The model requires no additional training and can produce fluent and contextually relevant outputs on a wide range of topics. By leveraging GPT-2, the system demonstrates the power of transformer-based language models in generating natural-sounding text with minimal setup.

Key Features:

Uses pretrained GPT-2 (no training needed)

Generates paragraph-length content based on user input

Easy-to-run Python script with customizable parameters

Suitable for creative writing, content ideation, or NLP experimentation

This project is ideal for learning how to integrate and use state-of-the-art language models in Python with just a few lines of code.

OUTPUT:
![Image](https://github.com/user-attachments/assets/6fcedbfc-b801-4653-b2c6-9ca45401be6b)
